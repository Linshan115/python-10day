from bs4 import BeautifulSoup     #网页解析，获取数据
import re       #正则表达式，进行文字匹配
import urllib.request,urllib.error      #制定url,获取网页数据
import xlwt     #进行excel操作
import sqlite3  #进行SQLite数据库操作

#爬取文件pdf地址


findpdf = re.compile(r'<a href="(.*)" target="_blank"')
findname = re.compile(r'target="_blank" title="(.*?)">')




def main():
    baseurl = "http://www.chinaclear.cn/zdjs/tjnb/center_datalist.shtml"
    datalist = geturl(baseurl)
    savepath = ".\\文件链接.xls"
    saveurl(datalist,savepath)


def askurl(baseurl):
    head = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/83.0.4103.106 Safari/537.36"
    }
    request = urllib.request.Request(baseurl, headers=head)
    html = ""
    try:
        responce = urllib.request.urlopen(request)
        html = responce.read().decode("utf-8")
    except urllib.error.URLError as e:
        if hasattr(e, "code"):
            print(e.code)
        if hasattr(e, "reason"):
            print(e.reason)
    return html


def geturl(baseurl):
    datalist = []
    html = askurl(baseurl)
    soup = BeautifulSoup(html,"html.parser")
    for item in soup.find_all('li'):
        data = []
        item = str(item)
        content = re.compile('<li>-(.*)</span></li>')
        item = re.findall(content,item)
        for i in item:
            i = str(i)
            pdf = re.findall(findpdf, i)[0]
            data.append(pdf)
            name = re.findall(findname,i)[0]
            data.append(name)
            datalist.append(data)
    print("爬取成功")
    return  datalist



def saveurl(datalist,savepath):
    workbook = xlwt.Workbook(encoding="utf-8")
    worksheet = workbook.add_sheet('sheet1')
    col = ("文件链接","文件名")
    for i in range(0,2):
        worksheet.write(0,i,col[i])
    for i in range(0,16):
        data = datalist[i]
        for j in range(0,2):
            worksheet.write(i+1,j,data[j] )

    workbook.save(savepath)





if __name__ == "__main__":      #当程序执行时
#调用函数
    main()
